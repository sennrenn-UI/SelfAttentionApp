# app.py
import streamlit as st
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import seaborn as sns
from janome.tokenizer import Tokenizer

st.set_page_config(page_title="Self-Attentionã€€å¯è¦–åŒ–ã‚¢ãƒ—ãƒª", page_icon="ğŸ’»")  

st.title("Self-Attention å¯è¦–åŒ–ã‚¢ãƒ—ãƒª")
st.markdown("æ–‡ç« ã‚’å…¥åŠ›ã—ã¦ã€Œè§£æã™ã‚‹ã€ã‚’æŠ¼ã™ã¨ã€Self-Attention ã®é‡ã¿ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚")

# æ–‡ç« å…¥åŠ›
text = st.text_area("æ–‡ç« ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", "åµã¯å»ã£ãŸãŒã€è¡—ã®å‚·è·¡ã¯æ·±ãæ®‹ã£ã¦ã„ãŸã€‚")

if st.button("è§£æã™ã‚‹"):
    if text.strip() == "":
        st.warning("æ–‡ç« ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    else:
        # Janome ã§å½¢æ…‹ç´ è§£æ
        t = Tokenizer()
        tokens = [token.surface for token in t.tokenize(text)]
        st.write("ãƒˆãƒ¼ã‚¯ãƒ³åˆ†å‰²çµæœ:", tokens)

        # Self-Attention è¨ˆç®—
        seq_len = len(tokens)
        torch.manual_seed(0)
        x = torch.randn(seq_len, 16)  # åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ 16

        Q = x
        K = x
        V = x

        d_k = Q.size(-1) ** 0.5
        scores = torch.matmul(Q, K.T) / d_k
        attn_weights = F.softmax(scores, dim=-1)
        output = torch.matmul(attn_weights, V)

        # å¯è¦–åŒ–
        fig, ax = plt.subplots(figsize=(6, 5))
        sns.heatmap(attn_weights.detach().numpy(),
                    annot=True,
                    xticklabels=tokens,
                    yticklabels=tokens,
                    cmap="YlGnBu",
                    ax=ax)
        plt.xlabel("Keyï¼ˆè¦‹ã¦ã‚‹å˜èªï¼‰")
        plt.ylabel("Queryï¼ˆæ³¨ç›®ã—ã¦ã‚‹å˜èªï¼‰")
        plt.title("Self-Attention Weights")
        st.pyplot(fig)

        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«å¿œã˜ã¦å‹•çš„ã«ã‚µã‚¤ã‚ºå¤‰æ›´ã—ãŸåˆ¥å›³
        fig2, ax2 = plt.subplots(figsize=(len(tokens), len(tokens)))
        st.write("attn_weights ã®å½¢:", attn_weights.shape)
        st.write(attn_weights)

        sns.heatmap(attn_weights.detach().numpy(), annot=True, xticklabels=tokens, yticklabels=tokens, cmap="YlGnBu",
                    ax=ax2)
        st.pyplot(fig2)
